{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model Implementation For Predicting Unreliable News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jackieglasheen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jackieglasheen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jackieglasheen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jackieglasheen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jackieglasheen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jackieglasheen/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jackieglasheen/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 12:40:49.094590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Additional BERT model packages \n",
    "# May need tensorflow==2.10  to run properly \n",
    "\n",
    "from transformers import BertModel, TFBertModel, BertTokenizer, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.ops.numpy_ops import np_utils\n",
    "from tensorflow.keras import regularizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data\n",
    "training_data = pd.read_csv(\n",
    "    '/Applications/Documents/Uchicago/2022_2023/3_Machine_Learning/project/all_preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "Complete final data cleaning steps, and then split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 1479     decade-old audio exposes then-senator hillary ...\n",
      "15559    he called nato obsolete he said germanys accep...\n",
      "6156     a new report from u s immigration and customs ...\n",
      "12987    more than five hundred and fifty zero people h...\n",
      "13964    by kurt nimmo blacklisted news in the video be...\n",
      "                               ...                        \n",
      "13123    marietta ga — jen cox bit her tongue for years...\n",
      "19648    on her february nineteenth episode of “full me...\n",
      "9845     an armed murder suspect allegedly opened fire ...\n",
      "10799    be the first to comment leave a reply click he...\n",
      "2732     chris mcdaniel mississippi state senator said ...\n",
      "Name: clean_text, Length: 16132, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "training_data.loc[:,'clean_text'] = training_data.loc[:,'clean_text'].fillna('')\n",
    "training_data['clean_text'] = training_data['clean_text'].str.replace(\"'\", \"\")\n",
    "training_data['clean_text'] = training_data['clean_text'].str.replace(\"’\", \"\")\n",
    "\n",
    "#Split Data into testing and training\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data.loc[:,'clean_text'], \n",
    "                                    training_data.label, test_size=0.20, random_state=0)\n",
    "print(X_train.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize BERT\n",
    "Initialize the BERT tokenizer and model using the pretrained \"bert-base-uncased\" version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a function to tokenize our text with the BERT Tokenizer. We chose a max_length of 75 (due to runtime concerns) and we truncate upon hitting that length. We return the token id and the attention mask, which informs whether the respective \"ID\" was padding or real text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(text_input):  \n",
    "    \"\"\"\n",
    "    Tokenize text for the BERT model\n",
    "    \n",
    "    Input (list of string): list of text to tokenize\n",
    "    Returns a \"BatchEncoding\" object holding the output of the tokenizers encoding. \n",
    "        Returns both the token encodings tensor constants(shape is (num_of_articles, 75))\n",
    "        and attention masks for the constants (shape is (num_of_articles, 75)).\n",
    "    \n",
    "    \"\"\"\n",
    "    return  bert_tokenizer(text = text_input, max_length = 75, truncation = True, padding = 'max_length',\n",
    "                    return_tensors = 'tf', return_attention_mask = True, return_token_type_ids = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize testing and training data\n",
    "# Caution: will take minutes to run\n",
    "X_train_token = bert_tokenize(list(X_train))\n",
    "X_test_token = bert_tokenize(list(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample return:  {'input_ids': <tf.Tensor: shape=(16132, 75), dtype=int32, numpy=\n",
      "array([[  101,  5476,  1011, ...,  1524,  2059,   102],\n",
      "       [  101,  2002,  2170, ...,  1998,  1996,   102],\n",
      "       [  101,  1037,  2047, ..., 12211,  2197,   102],\n",
      "       ...,\n",
      "       [  101,  2019,  4273, ..., 15106,  3573,   102],\n",
      "       [  101,  2022,  1996, ...,  4638,  2000,   102],\n",
      "       [  101,  3782, 11338, ...,  1996,  2627,   102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(16132, 75), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "print(\"sample return: \",X_train_token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Architecture\n",
    "We start by setting the appropriate shape for the first layer and initializing the BERT model. We then define the model architecture, including drop and dense layers, and then initialize the overall network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 75)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 75)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 75,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 768)          0           ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50)           38450       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 50)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            51          ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,520,741\n",
      "Trainable params: 109,520,741\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#tf.keras.backend.clear_session() #uncomment if you want to run tokens on different\n",
    "                                  #models without restarting the kernel\n",
    "    \n",
    "# set the shape of the first layer\n",
    "input_ids=Input(shape=75,dtype=tf.int32)\n",
    "input_mask=Input(shape=75,dtype=tf.int32)\n",
    "bert_layer=bert_model([input_ids,input_mask])[1]\n",
    "\n",
    "# Define the model layers, nest the layers\n",
    "x=Dropout(0.2)(bert_layer)\n",
    "x=Dense(50, activation=\"tanh\")(x)\n",
    "x=Dropout(0.2)(x)\n",
    "x=Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = Model(inputs=[input_ids, input_mask], outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Execution\n",
    "\n",
    "We use “Adam” for stochastic gradient descent optimization, and define the loss function as binary cross entropy. Then, run the model and produce training and test accuracy scores. We batch at 30 observations to allow the model to learn within an epoch, and limit the analysis to one epoch due to run-time concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=1e-05, epsilon=1e-08, decay=0.01, clipnorm=1.0)\n",
    "model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538/538 [==============================] - 9464s 18s/step - loss: 0.2211 - accuracy: 0.9101 - val_loss: 0.1264 - val_accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "# Caution: Will take hours to run\n",
    "output = model.fit(x = {'input_1':X_train_token['input_ids'],'input_2':X_train_token['attention_mask']}, y = y_train, epochs = 1, batch_size = 30,  validation_data=({'input_1':X_test_token['input_ids'],'input_2':X_test_token['attention_mask']},  y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1a563ca5215c41e3e2b7eadfe5702e6298b1434925ab9f9ba50081be0c38174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
